{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses two implicitly defined variables, namely: sc and spark. These are used to work with SPARK RDD and SPARK DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql as sql\n",
    "from pyspark.sql.types import StringType, FloatType, StructType, StructField, IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the paths where the various data files can be found.\n",
    "- traffic_path: path that points to the folder with the text files that contain the measurements (extracted from the XML files provided by the NDW)\n",
    "- weather_path: path that point to the folder with text files containing the weather measurements from the KNMI\n",
    "- linked_path: path to a text file that lists the three closest weather stations for every traffic camera\n",
    "- speed_path: text that specifies wich indices of the text file from `traffic_path` should be used to compute a weighted average\n",
    "- flow_path: similar to `speed_path`, only for the traffic flow values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_path = \"/home/thijs-gerrit/Desktop/p01-01-2017/*.txt\"\n",
    "weather_path = \"/home/thijs-gerrit/Documents/KNMI/*\"\n",
    "linked_path = \"/home/thijs-gerrit/Documents/station.txt\"\n",
    "speed_path = \"/home/thijs-gerrit/Documents/trafficspeed.txt\"\n",
    "flow_path = \"/home/thijs-gerrit/Documents/trafficflow.txt\"\n",
    "write_path = \"/home/thijs-gerrit/Documents/DataFrame\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions that split the data. These are made so that for every line only a single split has to be performed instead of multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_split(x):\n",
    "    temp = x.split(',')\n",
    "    return (temp[0], temp[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_traffic_data(x):\n",
    "    temp = x.split(\";\")\n",
    "    return (temp[1], [temp[0]] + temp[2:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the files from `speed_path` and `flow_path` and join them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed = sc.textFile(speed_path).map(lambda x: id_split(x))\n",
    "flow = sc.textFile(flow_path).map(lambda x: id_split(x))\n",
    "spd_flw_indices = flow.join(speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the data from the text files coming from `traffic_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_data = sc.textFile(traffic_path).map(lambda x: split_traffic_data(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the average speed per location and timestamp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function computes the weighted average speed which will function as the target for the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_speed(x):\n",
    "    \"\"\"\n",
    "    Compute a weighted average of the traffic speed\n",
    "    \"\"\"\n",
    "    spd_indices = list(map(int, x[1][0][1]))\n",
    "    flw_indices = list(map(int, x[1][0][0]))\n",
    "\n",
    "    flw_vals = []\n",
    "\n",
    "    if len(flw_indices) > 0:\n",
    "        for index in flw_indices:\n",
    "            try:\n",
    "                flw_val = float(x[1][1][index].split(\",\")[0])\n",
    "            except IndexError:\n",
    "                return -2.0\n",
    "                \n",
    "            if flw_val < 0:\n",
    "                flw_vals.append(0.0)\n",
    "            else:\n",
    "                flw_vals.append(flw_val)\n",
    "\n",
    "    total_flw = sum(flw_vals)\n",
    "\n",
    "    if total_flw > 0:\n",
    "        weights = []\n",
    "\n",
    "        for val in flw_vals:\n",
    "            weights.append(val / total_flw)\n",
    "\n",
    "        avg_spd = 0\n",
    "        if len(spd_indices) > 0:\n",
    "            for i in range(len(spd_indices)):\n",
    "                index = spd_indices[i]\n",
    "                spd_val = float(x[1][1][index].split(\",\")[1])\n",
    "                \n",
    "#                 if index+1 < len(spd_indices):\n",
    "                if i+1 <= len(spd_indices):\n",
    "                    avg_spd += spd_val * weights[i]\n",
    "            \n",
    "        return float(avg_spd)\n",
    "\n",
    "    else:\n",
    "        return -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp(x):\n",
    "    \"\"\"\n",
    "    Extract the timestamp from a string\n",
    "    \"\"\"\n",
    "    time_str = x[1][1][0]\n",
    "    \n",
    "    time_temp = time_str.split(\"T\")\n",
    "    \n",
    "    date = time_temp[0].split(\"-\")\n",
    "    time = time_temp[1].split(\":\")\n",
    "    \n",
    "    timestamp = date + time[:-1] #exclude seconds in the timestamp\n",
    "    \n",
    "    return list(map(int, timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speed_ID_pair(x):\n",
    "    \"\"\"\n",
    "    Pair the ID with the average speed and timestamp\n",
    "    \"\"\"\n",
    "    return [x[0]] + timestamp(x) + [avg_speed(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "average_speeds = spd_flw_indices.join(traffic_data).map(lambda x: speed_ID_pair(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert the `average_speeds` from an rdd to a DataFrame, but first create a schema (or structure) for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_speed_struct = StructType([\n",
    "    StructField('ID', StringType(), False),\n",
    "    StructField('Year', IntegerType(), False),\n",
    "    StructField('Month', IntegerType(), False),\n",
    "    StructField('Day', IntegerType(), False),\n",
    "    StructField('Hour', IntegerType(), False),\n",
    "    StructField('Minute', IntegerType(), False),\n",
    "    StructField('AvgSpeed', FloatType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_speed_frame = spark.createDataFrame(average_speeds, avg_speed_struct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the structures for the different DataFrames. These structures specify the column names, data type and if the column is allowed to empty (nullable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create structure for the traffic speed DataFrame\n",
    "speed_struct = StructType([\n",
    "    StructField(\"ID\", StringType(), False)\n",
    "])\n",
    "\n",
    "for i in range(7):\n",
    "    speed_struct.add(\"Index{0}\".format(i+1), IntegerType(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create structure for the traffic flow DataFrame\n",
    "flow_struct = StructType([\n",
    "    StructField(\"ID\", StringType(), False)\n",
    "])\n",
    "\n",
    "for i in range(7):\n",
    "    flow_struct.add(\"Index{0}\".format(i+1), IntegerType(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read traffic speed file\n",
    "speed_frame = spark.read.csv(path=speed_path, \n",
    "                               schema=speed_struct,\n",
    "                               sep=\",\")\n",
    "\n",
    "# read traffic flow file\n",
    "flow_frame = spark.read.csv(path=flow_path,\n",
    "                            schema=flow_struct,\n",
    "                            sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create structure for the weather DataFrame\n",
    "weather_struct = StructType([\n",
    "    StructField(\"STN\", IntegerType(), False),\n",
    "    StructField(\"Year\", IntegerType(), False),\n",
    "    StructField(\"Month\", IntegerType(), False),\n",
    "    StructField(\"Day\", IntegerType(), False),\n",
    "    StructField(\"FG\", FloatType(), True),\n",
    "    StructField(\"DR\", FloatType(), True),\n",
    "    StructField(\"RH\", FloatType(), True),\n",
    "    StructField(\"VVN\", FloatType(), True),\n",
    "    StructField(\"VVX\", FloatType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create structure for the linked station DataFrame\n",
    "linked_struct = StructType([\n",
    "    StructField(\"TrafficID\", StringType(), False),\n",
    "    StructField(\"TrafficLON\", FloatType(), False),\n",
    "    StructField(\"TrafficLAT\", FloatType(), False),\n",
    "])\n",
    "\n",
    "for i in range(3):\n",
    "    linked_struct.add(\"W{0}STN\".format(i + 1), IntegerType(), False)\n",
    "    linked_struct.add(\"W{0}Weight\".format(i + 1), FloatType(), False)\n",
    "    linked_struct.add(\"W{0}LON\".format(i + 1), FloatType(), False)\n",
    "    linked_struct.add(\"W{0}LAT\".format(i + 1), FloatType(), False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, the data files need to be loaded into DataFrames. Here we specify which structure (or schema) is expected and what delimter is used in the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read weather files\n",
    "weather_frame = spark.read.csv(path=weather_path,\n",
    "                               schema=weather_struct,\n",
    "                               sep=\";\")\n",
    "# read linked stations file\n",
    "linked_frame = spark.read.csv(path=linked_path,\n",
    "                              schema=linked_struct,\n",
    "                              sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now register the DataFrames as table's, so SQL queries can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_speed_frame.createOrReplaceTempView(\"Speed\")\n",
    "weather_frame.createOrReplaceTempView(\"Weather\")\n",
    "linked_frame.createOrReplaceTempView(\"LinkedStations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here an SQL Query is used to join the tables and filter out the rows that are not usable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT L.TrafficID AS ID, L.TrafficLON AS LON, L.TrafficLAT AS LAT,\n",
    "        S.Year AS Year, S.Month AS Month, S.Day AS Day, S.Hour AS Hour, S.Minute AS Minute,\n",
    "        S.AvgSpeed AS AvgSpeed,\n",
    "        (L.W1Weight * W1.FG + L.W2Weight * W2.FG + L.W3Weight * W3.FG) / 3 AS FG,\n",
    "        (L.W1Weight * W1.DR + L.W2Weight * W2.DR + L.W3Weight * W3.DR) / 3 AS DR,\n",
    "        (L.W1Weight * W1.RH + L.W2Weight * W2.RH + L.W3Weight * W3.RH) / 3 AS RH,\n",
    "        (L.W1Weight * W1.VVN + L.W2Weight * W2.VVN + L.W3Weight * W3.VVN) / 3 AS VVN,\n",
    "        (L.W1Weight * W1.VVX + L.W2Weight * W2.VVX + L.W3Weight * W3.VVX) / 3 AS VVX\n",
    "    FROM LinkedStations AS L\n",
    "    INNER JOIN Weather AS W1\n",
    "    ON L.W1STN = W1.STN\n",
    "    INNER JOIN Weather AS W2\n",
    "    ON L.W2STN = W2.STN\n",
    "    INNER JOIN Weather AS W3\n",
    "    ON L.W3STN = W3.STN\n",
    "    INNER JOIN Speed AS S\n",
    "    ON L.TrafficID = S.ID\n",
    "    WHERE S.Year = W1.Year AND S.Year = W2.Year AND S.Year = W3.Year AND\n",
    "        S.Month = W1.Month AND S.Month = W2.Month AND S.Month = W3.Month AND\n",
    "        S.Day = W1.Day AND S.Day = W2.Day AND S.Day = W3.Day AND\n",
    "        W1.FG IS NOT NULL  AND W2.FG IS NOT NULL AND W3.FG IS NOT NULL AND\n",
    "        W1.DR IS NOT NULL  AND W2.DR IS NOT NULL AND W3.DR IS NOT NULL AND\n",
    "        W1.RH IS NOT NULL  AND W2.RH IS NOT NULL AND W3.RH IS NOT NULL AND\n",
    "        W1.VVN IS NOT NULL  AND W2.VVN IS NOT NULL AND W3.VVN IS NOT NULL AND\n",
    "        W1.VVX IS NOT NULL  AND W2.VVX IS NOT NULL AND W3.VVX IS NOT NULL AND\n",
    "        S.AvgSpeed > -1\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(query).write.csv(\"/home/thijs-gerrit/Documents/DataFrame/\", mode='append', sep=\",\", header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
